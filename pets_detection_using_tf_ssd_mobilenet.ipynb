{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nulllpointer/custom_object_detection_colab/blob/master/pets_detection_using_tf_ssd_mobilenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIpOqkqUPUy9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dde4bc70-d79a-46c5-80d1-b50d5b9f1697"
      },
      "source": [
        "%cd\n",
        "import os\n",
        "\n",
        "!pip install --ignore-installed tf-nightly\n",
        "\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "\n",
        "!apt-get install -qq protobuf-compiler python-tk\n",
        "\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib PyDrive\n",
        "\n",
        "!pip install -q pycocotools\n",
        "\n",
        "%cd ~/models/research\n",
        "\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "os.environ['PYTHONPATH'] += ':/root/models/research/:/root/models/research/slim'\n",
        "\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/07/3d98fafa387bb0118b7b89a940ca9214b881f8499e0ba96a42f53eea6a05/tf_nightly-1.15.0.dev20190729-cp27-cp27mu-manylinux1_x86_64.whl (102.2MB)\n",
            "\u001b[K     |████████████████████████████████| 102.2MB 170kB/s \n",
            "\u001b[?25hCollecting grpcio>=1.8.6 (from tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/46/5d08b6e26748ed6f3b5e93d980ea5daa63c3a8200b2ad270645b0e2f9566/grpcio-1.22.0-cp27-cp27mu-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 30.4MB/s \n",
            "\u001b[?25hCollecting numpy<2.0,>=1.16.0 (from tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/c7/198496417c9c2f6226616cff7dedf2115a4f4d0276613bab842ec8ac1e23/numpy-1.16.4-cp27-cp27mu-manylinux1_x86_64.whl (17.0MB)\n",
            "\u001b[K     |████████████████████████████████| 17.0MB 31.8MB/s \n",
            "\u001b[?25hCollecting backports.weakref>=1.0rc1; python_version < \"3.4\" (from tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
            "Collecting tb-nightly<1.16.0a0,>=1.15.0a0 (from tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/0f/9354f65421b82e0d5cdcf13d5e03bfb5f8a1dc30e19fc57717f77a98e89c/tb_nightly-1.15.0a20190729-py2-none-any.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 28.4MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1 (from tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
            "Collecting protobuf>=3.6.1 (from tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/bf/d7f8bc958f9eb4661498cde9f7e630da863e43278222d46105712dde4e13/protobuf-3.9.0-cp27-cp27mu-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 39.1MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 18.4MB/s \n",
            "\u001b[?25hCollecting gast>=0.2.0 (from tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting keras-applications>=1.0.8 (from tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/56/4bcec5a8d9503a87e58e814c4e32ac2b32c37c685672c30bc8c54c6e478a/Keras_Applications-1.0.8.tar.gz (289kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 33.8MB/s \n",
            "\u001b[?25hCollecting wheel (from tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/10/44230dd6bf3563b8f227dbf344c908d412ad2ff48066476672f3a72e174e/wheel-0.33.4-py2.py3-none-any.whl\n",
            "Collecting functools32>=3.2.3 (from tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/60/6ac26ad05857c601308d8fb9e87fa36d0ebf889423f47c3502ef034365db/functools32-3.2.3-2.tar.gz\n",
            "Collecting six>=1.10.0 (from tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting tf-estimator-nightly (from tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/cf/a94ee44b408cdfb76af10381ed333a23d2a1522463ea06d43ba76db54df8/tf_estimator_nightly-1.14.0.dev2019072901-py2.py3-none-any.whl (501kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 41.6MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.7.0 (from tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 26.5MB/s \n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2 (from tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 18.4MB/s \n",
            "\u001b[?25hCollecting termcolor>=1.1.0 (from tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
            "Collecting google-pasta>=0.1.6 (from tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/95/d41cd87d147742ef72d5d1dc317318486e3fbffdadf24a60e70dedf01d56/google_pasta-0.1.7-py2-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 19.9MB/s \n",
            "\u001b[?25hCollecting enum34>=1.1.6; python_version < \"3.4\" (from tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl\n",
            "Collecting mock>=2.0.0 (from tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Collecting astor>=0.6.0 (from tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
            "Collecting futures>=2.2.0; python_version < \"3.2\" (from grpcio>=1.8.6->tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/d8/a6/f46ae3f1da0cd4361c344888f59ec2f5785e69c872e175a748ef6071cdb5/futures-3.3.0-py2-none-any.whl\n",
            "Collecting setuptools>=41.0.0 (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl (575kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 41.0MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15 (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/ab/d3bed6b92042622d24decc7aadc8877badf18aeca1571045840ad4956d3f/Werkzeug-0.15.5-py2.py3-none-any.whl (328kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 43.5MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8 (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 25.1MB/s \n",
            "\u001b[?25hCollecting h5py (from keras-applications>=1.0.8->tf-nightly)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/08/27e4e9a369321862ffdce80ff1770553e9daec65d98befb2e14e7478b698/h5py-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 37.0MB/s \n",
            "\u001b[?25hCollecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tf-nightly)\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: wrapt, gast, keras-applications, functools32, absl-py, opt-einsum, termcolor\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for keras-applications (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/f2/5d/2689b5547f32c4e258c3b7ccbe7f1d0f2afbb84fb01e830792\n",
            "  Building wheel for functools32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/18/32/77a1030457155606ba5e3ec3a8a57132b1a04b1c4f765177b2\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
            "Successfully built wrapt gast keras-applications functools32 absl-py opt-einsum termcolor\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: enum34, six, futures, grpcio, numpy, backports.weakref, setuptools, protobuf, werkzeug, absl-py, wheel, markdown, tb-nightly, wrapt, keras-preprocessing, gast, h5py, keras-applications, functools32, tf-estimator-nightly, opt-einsum, termcolor, google-pasta, funcsigs, mock, astor, tf-nightly\n",
            "Successfully installed absl-py-0.7.1 astor-0.8.0 backports.weakref-1.0.post1 enum34-1.1.6 funcsigs-1.0.2 functools32-3.2.3.post2 futures-3.3.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.22.0 h5py-2.9.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 mock-3.0.5 numpy-1.16.4 opt-einsum-2.3.2 protobuf-3.9.0 setuptools-41.0.1 six-1.12.0 tb-nightly-1.15.0a20190729 termcolor-1.1.0 tf-estimator-nightly-1.14.0.dev2019072901 tf-nightly-1.15.0.dev20190729 werkzeug-0.15.5 wheel-0.33.4 wrapt-1.11.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "backports",
                  "concurrent",
                  "enum",
                  "google",
                  "grpc",
                  "numpy",
                  "pkg_resources",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n",
            "/root/models/research\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0729 19:47:44.090457 140127443171200 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0729 19:47:45.087886 140127443171200 module_wrapper.py:136] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0729 19:47:45.096231 140127443171200 module_wrapper.py:136] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Running tests under Python 2.7.15: /usr/bin/python2\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[       OK ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[       OK ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[       OK ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[       OK ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTest.test_session\n",
            "[  SKIPPED ] ModelBuilderTest.test_session\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[       OK ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 16 tests in 0.166s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvUaPeLeP1QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "from os import path\n",
        "%mkdir -p ~/datalab \n",
        "%cd ~/datalab\n",
        "uploaded = files.upload()  \n",
        "for name, data in uploaded.items():\n",
        "  with open('label_map.pbtxt', 'wb') as f:\n",
        "    f.write(data)\n",
        "    f.close()\n",
        "    print('saved file ' + name)\n",
        "!cat ~/datalab/label_map.pbtxt    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63x4LtUOP7Oe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "from shutil import copy\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "\"\"\"creates a directory and downloads the datalset from gdrive\"\"\"\n",
        "\n",
        "%cd ~/datalab\n",
        "\n",
        "fileId = '1rtChiD59mt2p93zU34DN_OSYgnza6reG'\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "fileName = fileId + '.zip'\n",
        "downloaded = drive.CreateFile({'id': fileId})\n",
        "downloaded.GetContentFile(fileName)\n",
        "ds = ZipFile(fileName)\n",
        "ds.extractall()\n",
        "os.remove(fileName)\n",
        "print('Extracted zip file ' + fileName)\n",
        "\n",
        "image_files=os.listdir('images')\n",
        "im_files=[x.split('.')[0] for x in image_files]\n",
        "with open('annotations/trainval.txt', 'w') as text_file:\n",
        "  for row in im_files:\n",
        "    text_file.write(row + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVHItykBP8kL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "%cd ~/datalab/annotations\n",
        "!mkdir -p trimaps\n",
        "image = Image.new('RGB', (640, 480))\n",
        "\n",
        "for filename in os.listdir('xmls'):\n",
        "  filename = os.path.splitext(filename)[0]\n",
        "  image.save('trimaps/' + filename + '.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I12zKvsZQAlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ~/datalab\n",
        "\n",
        "!python ~/models/research/object_detection/dataset_tools/create_pet_tf_record.py --label_map_path=label_map.pbtxt --data_dir=. --output_dir=. --num_shards=1\n",
        "\n",
        "!mv pet_faces_train.record-00000-of-00001 tf_train.record\n",
        "\n",
        "!mv pet_faces_val.record-00000-of-00001 tf_val.record"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qdwBNOgQGRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib\n",
        "import tarfile\n",
        "\n",
        "\n",
        "# Get list of models from https://github.com/tensorflow/models/blob/fe748d4a4a1576b57c279014ac0ceb47344399c4/research/object_detection/g3doc/detection_model_zoo.md\n",
        "\n",
        "%cd ~/datalab\n",
        "MODEL= 'ssd_inception_v2_coco_2018_01_28'\n",
        "# MODEL = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = 'pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "  opener = urllib.URLopener()\n",
        "  opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "  shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms3gJVCwQMh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from google.protobuf import text_format\n",
        "from object_detection.protos import pipeline_pb2\n",
        "\n",
        "\n",
        "# !cat /root/models/research/object_detection/samples/configs/ssd_inception_v2_pets.config\n",
        "!cp -r ~/models/research/object_detection /usr/local/lib/python2.7/dist-packages/\n",
        "\n",
        "filename = '/root/models/research/object_detection/samples/configs/ssd_inception_v2_pets.config'\n",
        "\n",
        "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()                                                                                                                                                                                                          \n",
        "\n",
        "with tf.gfile.GFile(filename, \"r\") as f:                                                                                                                                                                                                                     \n",
        "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
        "    text_format.Merge(proto_str, pipeline_config)                                                                                                                                                                                                                 \n",
        "\n",
        "pipeline_config.train_config.fine_tune_checkpoint = '/root/datalab/pretrained_model/model.ckpt'\n",
        "pipeline_config.train_input_reader[0].label_map_path = '/root/datalab/label_map.pbtxt'\n",
        "pipeline_config.train_input_reader.tf_record_input_reader.input_path[0] = '/root/datalab/tf_train.record'\n",
        "pipeline_config.eval_input_reader[0].label_map_path = '/root/datalab/label_map.pbtxt'\n",
        "pipeline_config.eval_input_reader.tf_record_input_reader.input_path[0] = '/root/datalab/tf_val.record'\n",
        "    \n",
        "\n",
        "\n",
        "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
        "with tf.gfile.Open(filename, \"wb\") as f:                                                                                                                                                                                                                       \n",
        "    f.write(config_text)                                                                                                                                                                                                                                          \n",
        "\n",
        "\n",
        "!cat /root/models/research/object_detection/samples/configs/ssd_inception_v2_pets.config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Q8j7ngQN3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ~/datalab\n",
        "!pwd\n",
        "import tensorflow as tf\n",
        "\n",
        "PATH_TO_BE_CONFIGURED='~/datalab'\n",
        "\n",
        "!mkdir -p eval_logs\n",
        "!python /root/models/research/object_detection/samples/configs/ssd_inception_v2_pets.config \\\n",
        "     --logtostderr \\\n",
        "     --pipeline_config_path=ssd_mobilenet_v1_pets.config \\\n",
        "     --checkpoint_dir=trained \\\n",
        "     --eval_dir=eval_logs \n",
        "\n",
        "%cd object_detection/ssd_model\n",
        "%tensorboard --logdir .\n",
        "\n",
        "!python ~/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path=/root/models/research/object_detection/samples/configs/ssd_inception_v2_pets.config \\\n",
        "    --model_dir=/root/datalab/trained \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps=3000 \\\n",
        "    --num_eval_steps=1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6ytmSi8RakV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ~/datalab\n",
        "\n",
        "lst = os.listdir('trained')\n",
        "lf = filter(lambda k: 'model.ckpt-' in k, lst)\n",
        "print(lf)\n",
        "\n",
        "last_model = sorted(lf)[-1].replace('.meta', '')\n",
        "!python ~/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=/root/models/research/object_detection/samples/configs/ssd_inception_v2_pets.config \\\n",
        "    --output_directory=fine_tuned_model \\\n",
        "    --trained_checkpoint_prefix=trained/$last_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UOh2RhbRbn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ~/datalab\n",
        "\n",
        "from google.colab import files\n",
        "from os import path\n",
        "\n",
        "uploaded = files.upload()\n",
        "  \n",
        "for name, data in uploaded.items():\n",
        "  with open('image1.jpg', 'wb') as f:\n",
        "    f.write(data)\n",
        "    f.close()\n",
        "    print('saved file ' + name)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F0A_L8RRgw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ~/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "# if tf.__version__ < '1.4.0':\n",
        "#   raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from utils import label_map_util\n",
        "\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "\n",
        "# What model to download.\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = '/root/datalab/fine_tuned_model' + '/frozen_inference_graph.pb'\n",
        "\n",
        "\n",
        "PATH_TO_CKPT = '/root/datalab/ssd_mobilenet_v1_coco_2018_01_28' + '/frozen_inference_graph.pb'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = os.path.join('/root/datalab', 'label_map.pbtxt')\n",
        "\n",
        "NUM_CLASSES = 37\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.GraphDef()\n",
        "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "print('hero')\n",
        "    \n",
        "with tf.Session() as sess:\n",
        "  # `sess.graph` provides access to the graph used in a `tf.Session`.\n",
        "  writer = tf.summary.FileWriter(\"./hero\", sess.graph)    \n",
        "    \n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
        "PATH_TO_TEST_IMAGES_DIR = '/root/datalab/'\n",
        "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 2) ]\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      # Get handles to input and output tensors\n",
        "      ops = tf.get_default_graph().get_operations()\n",
        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "      tensor_dict = {}\n",
        "      for key in [\n",
        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
        "          'detection_classes', 'detection_masks'\n",
        "      ]:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "              tensor_name)\n",
        "      if 'detection_masks' in tensor_dict:\n",
        "        # The following processing is only for single image\n",
        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "        detection_masks_reframed = tf.cast(\n",
        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "        # Follow the convention by adding back the batch dimension\n",
        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "            detection_masks_reframed, 0)\n",
        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "      # Run inference\n",
        "      output_dict = sess.run(tensor_dict,\n",
        "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "      output_dict['detection_classes'] = output_dict[\n",
        "          'detection_classes'][0].astype(np.uint8)\n",
        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "      if 'detection_masks' in output_dict:\n",
        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "        \n",
        "  return output_dict\n",
        "\n",
        "\n",
        "\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  image = Image.open(image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "  plt.figure(figsize=IMAGE_SIZE)\n",
        "  plt.imshow(image_np)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQjg4YYXRyN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For TF lite\n",
        "\n",
        "%cd ~/datalab\n",
        "lst = os.listdir('trained')\n",
        "lf = filter(lambda k: 'model.ckpt-' in k, lst)\n",
        "print(lf)\n",
        "\n",
        "last_model = sorted(lf)[-1].replace('.meta', '')\n",
        "!python ~/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
        "    --pipeline_config_path=/root/models/research/object_detection/samples/configs/ssd_inception_v2_pets.config \\\n",
        "    --trained_checkpoint_prefix=trained/$last_model \\\n",
        "    --output_directory=tflite_fine_tuned_model \\\n",
        "    --max_detections 3"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}